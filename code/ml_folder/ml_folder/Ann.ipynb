{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDuwbwg2zj8t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import estimator as tf_estimator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect to tour dags hub account replace username and project name in below code ,\n",
        "Add your Dagshub key when propmted"
      ],
      "metadata": {
        "id": "fQhR8xnT2v36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow --quiet\n",
        "\n",
        "import mlflow\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = 'vedantd1998'\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('enter access token')\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = 'ML'\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygyXuhaV4Lwr",
        "outputId": "183c8f7a-6518-44ee-a3c1-aff0fe394f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.5 MB 186 kB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 539 kB/s \n",
            "\u001b[K     |████████████████████████████████| 210 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.5 MB/s \n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "enter access token··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import keras"
      ],
      "metadata": {
        "id": "-rJcz-Oa8yVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "train_df = pd.read_csv(r'/content/test_data.csv',index_col=0)\n",
        "X=train_df.drop('credit_card_default',axis=1)\n",
        "y=train_df['credit_card_default']\n",
        "\n",
        "\n",
        "smote = ADASYN(n_neighbors=5)\n",
        "X_train, y_train = smote.fit_resample(X, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(r'/content/test_data.csv',index_col=0)\n",
        "X_test=test_df.drop('credit_card_default',axis=1)\n",
        "y_test=test_df['credit_card_default']\n"
      ],
      "metadata": {
        "id": "2nVpYo9b0Skr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with mlflow.start_run(run_name=\"MLflow on Colab\"):\n",
        "  \n",
        "\n",
        "  tf.random.set_seed(2022)\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(13, activation='relu',input_shape=(X_train.shape[1], )),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(8, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "  ])\n",
        "\n",
        "\n",
        "\n",
        "  model.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['Precision'])\n",
        "\n",
        "  from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "  monitor = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, verbose=2, mode='auto',\n",
        "          restore_best_weights=True)\n",
        "  history2 = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=[monitor],verbose=2,epochs=500)\n",
        "\n",
        "  y_pred_prob = model.predict(X_test)\n",
        "  y_pred = np.where(y_pred_prob>=0.5,1,0)\n",
        "\n",
        "  from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss, precision_score, recall_score\n",
        "\n",
        "\n",
        "  mlflow.log_metric(\"accuracy\", accuracy_score(y_test,y_pred))\n",
        "  mlflow.log_metric(\"f1\", f1_score(y_test,y_pred))\n",
        "  mlflow.log_metric(\"precsion\", precision_score(y_test,y_pred))\n",
        "  mlflow.log_metric(\"recall\", recall_score(y_test,y_pred))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLKu-vQp82R7",
        "outputId": "c14846ce-4f57-45c9-d296-0ccdd22c4a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "786/786 - 4s - loss: 0.4260 - precision: 0.7842 - val_loss: 0.1935 - val_precision: 0.4665 - 4s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "786/786 - 2s - loss: 0.2055 - precision: 0.9065 - val_loss: 0.1537 - val_precision: 0.5411 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "786/786 - 2s - loss: 0.1528 - precision: 0.9283 - val_loss: 0.1504 - val_precision: 0.5757 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "786/786 - 2s - loss: 0.1370 - precision: 0.9336 - val_loss: 0.1366 - val_precision: 0.5960 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "786/786 - 2s - loss: 0.1268 - precision: 0.9381 - val_loss: 0.1266 - val_precision: 0.6174 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "786/786 - 2s - loss: 0.1217 - precision: 0.9403 - val_loss: 0.1269 - val_precision: 0.6204 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "786/786 - 2s - loss: 0.1148 - precision: 0.9422 - val_loss: 0.1257 - val_precision: 0.6294 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "786/786 - 2s - loss: 0.1138 - precision: 0.9428 - val_loss: 0.1201 - val_precision: 0.6322 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "786/786 - 2s - loss: 0.1152 - precision: 0.9462 - val_loss: 0.1257 - val_precision: 0.6340 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "786/786 - 2s - loss: 0.1084 - precision: 0.9457 - val_loss: 0.1259 - val_precision: 0.6343 - 2s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "786/786 - 2s - loss: 0.1094 - precision: 0.9469 - val_loss: 0.1180 - val_precision: 0.6458 - 2s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "786/786 - 2s - loss: 0.1063 - precision: 0.9472 - val_loss: 0.1177 - val_precision: 0.6434 - 2s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "786/786 - 2s - loss: 0.1052 - precision: 0.9484 - val_loss: 0.1186 - val_precision: 0.6452 - 2s/epoch - 3ms/step\n",
            "Epoch 14/500\n",
            "786/786 - 2s - loss: 0.1056 - precision: 0.9477 - val_loss: 0.1110 - val_precision: 0.6585 - 2s/epoch - 3ms/step\n",
            "Epoch 15/500\n",
            "786/786 - 2s - loss: 0.1031 - precision: 0.9491 - val_loss: 0.1160 - val_precision: 0.6523 - 2s/epoch - 3ms/step\n",
            "Epoch 16/500\n",
            "786/786 - 2s - loss: 0.1012 - precision: 0.9497 - val_loss: 0.1291 - val_precision: 0.6371 - 2s/epoch - 3ms/step\n",
            "Epoch 17/500\n",
            "786/786 - 2s - loss: 0.1015 - precision: 0.9494 - val_loss: 0.1226 - val_precision: 0.6432 - 2s/epoch - 3ms/step\n",
            "Epoch 18/500\n",
            "786/786 - 2s - loss: 0.1008 - precision: 0.9500 - val_loss: 0.1174 - val_precision: 0.6500 - 2s/epoch - 3ms/step\n",
            "Epoch 19/500\n",
            "786/786 - 2s - loss: 0.0997 - precision: 0.9502 - val_loss: 0.1140 - val_precision: 0.6638 - 2s/epoch - 3ms/step\n",
            "Epoch 20/500\n",
            "786/786 - 2s - loss: 0.0996 - precision: 0.9499 - val_loss: 0.1154 - val_precision: 0.6627 - 2s/epoch - 3ms/step\n",
            "Epoch 21/500\n",
            "786/786 - 2s - loss: 0.1015 - precision: 0.9488 - val_loss: 0.1111 - val_precision: 0.6653 - 2s/epoch - 3ms/step\n",
            "Epoch 22/500\n",
            "786/786 - 3s - loss: 0.0979 - precision: 0.9506 - val_loss: 0.1120 - val_precision: 0.6675 - 3s/epoch - 3ms/step\n",
            "Epoch 23/500\n",
            "786/786 - 2s - loss: 0.0969 - precision: 0.9515 - val_loss: 0.1166 - val_precision: 0.6576 - 2s/epoch - 3ms/step\n",
            "Epoch 24/500\n",
            "786/786 - 2s - loss: 0.0993 - precision: 0.9515 - val_loss: 0.1126 - val_precision: 0.6633 - 2s/epoch - 3ms/step\n",
            "Epoch 25/500\n",
            "786/786 - 2s - loss: 0.0955 - precision: 0.9528 - val_loss: 0.1075 - val_precision: 0.6744 - 2s/epoch - 3ms/step\n",
            "Epoch 26/500\n",
            "786/786 - 3s - loss: 0.0935 - precision: 0.9533 - val_loss: 0.1116 - val_precision: 0.6719 - 3s/epoch - 3ms/step\n",
            "Epoch 27/500\n",
            "786/786 - 2s - loss: 0.0929 - precision: 0.9530 - val_loss: 0.1103 - val_precision: 0.6775 - 2s/epoch - 3ms/step\n",
            "Epoch 28/500\n",
            "786/786 - 2s - loss: 0.0919 - precision: 0.9551 - val_loss: 0.1114 - val_precision: 0.6673 - 2s/epoch - 3ms/step\n",
            "Epoch 29/500\n",
            "786/786 - 2s - loss: 0.0909 - precision: 0.9546 - val_loss: 0.1133 - val_precision: 0.6789 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "786/786 - 2s - loss: 0.1005 - precision: 0.9526 - val_loss: 0.1086 - val_precision: 0.6711 - 2s/epoch - 3ms/step\n",
            "Epoch 31/500\n",
            "786/786 - 3s - loss: 0.0918 - precision: 0.9545 - val_loss: 0.1097 - val_precision: 0.6769 - 3s/epoch - 3ms/step\n",
            "Epoch 32/500\n",
            "786/786 - 2s - loss: 0.0908 - precision: 0.9546 - val_loss: 0.1076 - val_precision: 0.6817 - 2s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "786/786 - 2s - loss: 0.0910 - precision: 0.9554 - val_loss: 0.1100 - val_precision: 0.6738 - 2s/epoch - 3ms/step\n",
            "Epoch 34/500\n",
            "786/786 - 3s - loss: 0.0910 - precision: 0.9545 - val_loss: 0.1063 - val_precision: 0.6798 - 3s/epoch - 3ms/step\n",
            "Epoch 35/500\n",
            "786/786 - 3s - loss: 0.0902 - precision: 0.9561 - val_loss: 0.1047 - val_precision: 0.6887 - 3s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "786/786 - 2s - loss: 0.0907 - precision: 0.9556 - val_loss: 0.0993 - val_precision: 0.6900 - 2s/epoch - 3ms/step\n",
            "Epoch 37/500\n",
            "786/786 - 2s - loss: 0.0902 - precision: 0.9564 - val_loss: 0.1039 - val_precision: 0.6883 - 2s/epoch - 3ms/step\n",
            "Epoch 38/500\n",
            "786/786 - 3s - loss: 0.0911 - precision: 0.9551 - val_loss: 0.1025 - val_precision: 0.6947 - 3s/epoch - 4ms/step\n",
            "Epoch 39/500\n",
            "786/786 - 2s - loss: 0.0907 - precision: 0.9557 - val_loss: 0.1088 - val_precision: 0.6842 - 2s/epoch - 3ms/step\n",
            "Epoch 40/500\n",
            "786/786 - 2s - loss: 0.0891 - precision: 0.9559 - val_loss: 0.1030 - val_precision: 0.6908 - 2s/epoch - 3ms/step\n",
            "Epoch 41/500\n",
            "786/786 - 3s - loss: 0.0908 - precision: 0.9553 - val_loss: 0.1010 - val_precision: 0.6919 - 3s/epoch - 3ms/step\n",
            "Epoch 42/500\n",
            "786/786 - 2s - loss: 0.0881 - precision: 0.9564 - val_loss: 0.1014 - val_precision: 0.6952 - 2s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "786/786 - 2s - loss: 0.0887 - precision: 0.9569 - val_loss: 0.1030 - val_precision: 0.6906 - 2s/epoch - 3ms/step\n",
            "Epoch 44/500\n",
            "786/786 - 2s - loss: 0.0887 - precision: 0.9565 - val_loss: 0.1043 - val_precision: 0.6913 - 2s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "786/786 - 2s - loss: 0.0902 - precision: 0.9574 - val_loss: 0.1073 - val_precision: 0.6908 - 2s/epoch - 3ms/step\n",
            "Epoch 46/500\n",
            "786/786 - 2s - loss: 0.0913 - precision: 0.9568 - val_loss: 0.0990 - val_precision: 0.6974 - 2s/epoch - 3ms/step\n",
            "Epoch 47/500\n",
            "786/786 - 2s - loss: 0.0859 - precision: 0.9585 - val_loss: 0.1084 - val_precision: 0.6865 - 2s/epoch - 3ms/step\n",
            "Epoch 48/500\n",
            "786/786 - 2s - loss: 0.0873 - precision: 0.9566 - val_loss: 0.1031 - val_precision: 0.6965 - 2s/epoch - 3ms/step\n",
            "Epoch 49/500\n",
            "786/786 - 2s - loss: 0.0862 - precision: 0.9577 - val_loss: 0.1011 - val_precision: 0.6976 - 2s/epoch - 3ms/step\n",
            "Epoch 50/500\n",
            "786/786 - 2s - loss: 0.0884 - precision: 0.9567 - val_loss: 0.1063 - val_precision: 0.6859 - 2s/epoch - 3ms/step\n",
            "Epoch 51/500\n",
            "786/786 - 3s - loss: 0.0888 - precision: 0.9573 - val_loss: 0.1020 - val_precision: 0.6987 - 3s/epoch - 4ms/step\n",
            "Epoch 52/500\n",
            "786/786 - 2s - loss: 0.0885 - precision: 0.9565 - val_loss: 0.1059 - val_precision: 0.6937 - 2s/epoch - 3ms/step\n",
            "Epoch 53/500\n",
            "786/786 - 2s - loss: 0.0871 - precision: 0.9576 - val_loss: 0.1016 - val_precision: 0.6980 - 2s/epoch - 3ms/step\n",
            "Epoch 54/500\n",
            "786/786 - 2s - loss: 0.0864 - precision: 0.9574 - val_loss: 0.1030 - val_precision: 0.6977 - 2s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "786/786 - 2s - loss: 0.0865 - precision: 0.9577 - val_loss: 0.1045 - val_precision: 0.6965 - 2s/epoch - 3ms/step\n",
            "Epoch 56/500\n",
            "786/786 - 2s - loss: 0.0850 - precision: 0.9584 - val_loss: 0.0964 - val_precision: 0.7003 - 2s/epoch - 3ms/step\n",
            "Epoch 57/500\n",
            "786/786 - 2s - loss: 0.0863 - precision: 0.9578 - val_loss: 0.0998 - val_precision: 0.7041 - 2s/epoch - 3ms/step\n",
            "Epoch 58/500\n",
            "786/786 - 2s - loss: 0.0891 - precision: 0.9579 - val_loss: 0.1007 - val_precision: 0.6995 - 2s/epoch - 3ms/step\n",
            "Epoch 59/500\n",
            "786/786 - 3s - loss: 0.0841 - precision: 0.9580 - val_loss: 0.1036 - val_precision: 0.7020 - 3s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "786/786 - 3s - loss: 0.0864 - precision: 0.9574 - val_loss: 0.0969 - val_precision: 0.7079 - 3s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "786/786 - 2s - loss: 0.0871 - precision: 0.9574 - val_loss: 0.1059 - val_precision: 0.6940 - 2s/epoch - 3ms/step\n",
            "Epoch 62/500\n",
            "786/786 - 3s - loss: 0.0869 - precision: 0.9571 - val_loss: 0.1013 - val_precision: 0.6996 - 3s/epoch - 3ms/step\n",
            "Epoch 63/500\n",
            "786/786 - 2s - loss: 0.0960 - precision: 0.9582 - val_loss: 0.1025 - val_precision: 0.6939 - 2s/epoch - 3ms/step\n",
            "Epoch 64/500\n",
            "786/786 - 2s - loss: 0.0852 - precision: 0.9583 - val_loss: 0.0949 - val_precision: 0.7119 - 2s/epoch - 3ms/step\n",
            "Epoch 65/500\n",
            "786/786 - 2s - loss: 0.0858 - precision: 0.9583 - val_loss: 0.1015 - val_precision: 0.6953 - 2s/epoch - 3ms/step\n",
            "Epoch 66/500\n",
            "786/786 - 2s - loss: 0.0862 - precision: 0.9583 - val_loss: 0.1020 - val_precision: 0.7020 - 2s/epoch - 3ms/step\n",
            "Epoch 67/500\n",
            "786/786 - 2s - loss: 0.0855 - precision: 0.9579 - val_loss: 0.1026 - val_precision: 0.7072 - 2s/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "786/786 - 2s - loss: 0.0868 - precision: 0.9575 - val_loss: 0.1037 - val_precision: 0.6914 - 2s/epoch - 3ms/step\n",
            "Epoch 69/500\n",
            "786/786 - 2s - loss: 0.0861 - precision: 0.9576 - val_loss: 0.1034 - val_precision: 0.7070 - 2s/epoch - 3ms/step\n",
            "Epoch 70/500\n",
            "786/786 - 2s - loss: 0.0885 - precision: 0.9582 - val_loss: 0.1040 - val_precision: 0.6999 - 2s/epoch - 3ms/step\n",
            "Epoch 71/500\n",
            "786/786 - 2s - loss: 0.0880 - precision: 0.9563 - val_loss: 0.1018 - val_precision: 0.6958 - 2s/epoch - 3ms/step\n",
            "Epoch 72/500\n",
            "786/786 - 2s - loss: 0.0862 - precision: 0.9575 - val_loss: 0.1044 - val_precision: 0.7003 - 2s/epoch - 3ms/step\n",
            "Epoch 73/500\n",
            "786/786 - 2s - loss: 0.0861 - precision: 0.9574 - val_loss: 0.0974 - val_precision: 0.7066 - 2s/epoch - 3ms/step\n",
            "Epoch 74/500\n",
            "786/786 - 2s - loss: 0.0857 - precision: 0.9586 - val_loss: 0.0943 - val_precision: 0.7060 - 2s/epoch - 3ms/step\n",
            "Epoch 75/500\n",
            "786/786 - 2s - loss: 0.0859 - precision: 0.9585 - val_loss: 0.0989 - val_precision: 0.7036 - 2s/epoch - 3ms/step\n",
            "Epoch 76/500\n",
            "786/786 - 2s - loss: 0.0855 - precision: 0.9580 - val_loss: 0.1058 - val_precision: 0.6989 - 2s/epoch - 3ms/step\n",
            "Epoch 77/500\n",
            "786/786 - 2s - loss: 0.0850 - precision: 0.9587 - val_loss: 0.0992 - val_precision: 0.7041 - 2s/epoch - 3ms/step\n",
            "Epoch 78/500\n",
            "786/786 - 2s - loss: 0.1049 - precision: 0.9591 - val_loss: 0.0973 - val_precision: 0.7046 - 2s/epoch - 3ms/step\n",
            "Epoch 79/500\n",
            "786/786 - 2s - loss: 0.0868 - precision: 0.9578 - val_loss: 0.1039 - val_precision: 0.7007 - 2s/epoch - 3ms/step\n",
            "Epoch 80/500\n",
            "786/786 - 2s - loss: 0.0867 - precision: 0.9579 - val_loss: 0.0950 - val_precision: 0.7106 - 2s/epoch - 3ms/step\n",
            "Epoch 81/500\n",
            "786/786 - 2s - loss: 0.0855 - precision: 0.9593 - val_loss: 0.0918 - val_precision: 0.7192 - 2s/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "786/786 - 2s - loss: 0.0861 - precision: 0.9586 - val_loss: 0.0968 - val_precision: 0.7066 - 2s/epoch - 3ms/step\n",
            "Epoch 83/500\n",
            "786/786 - 2s - loss: 0.0847 - precision: 0.9596 - val_loss: 0.1012 - val_precision: 0.7036 - 2s/epoch - 3ms/step\n",
            "Epoch 84/500\n",
            "786/786 - 2s - loss: 0.0854 - precision: 0.9583 - val_loss: 0.0956 - val_precision: 0.7050 - 2s/epoch - 3ms/step\n",
            "Epoch 85/500\n",
            "786/786 - 2s - loss: 0.0854 - precision: 0.9587 - val_loss: 0.1019 - val_precision: 0.7022 - 2s/epoch - 3ms/step\n",
            "Epoch 86/500\n",
            "786/786 - 2s - loss: 0.0850 - precision: 0.9592 - val_loss: 0.0951 - val_precision: 0.7177 - 2s/epoch - 3ms/step\n",
            "Epoch 87/500\n",
            "786/786 - 2s - loss: 0.0827 - precision: 0.9601 - val_loss: 0.0959 - val_precision: 0.7147 - 2s/epoch - 3ms/step\n",
            "Epoch 88/500\n",
            "786/786 - 2s - loss: 0.0868 - precision: 0.9581 - val_loss: 0.1008 - val_precision: 0.7094 - 2s/epoch - 3ms/step\n",
            "Epoch 89/500\n",
            "786/786 - 2s - loss: 0.0873 - precision: 0.9582 - val_loss: 0.1017 - val_precision: 0.7047 - 2s/epoch - 3ms/step\n",
            "Epoch 90/500\n",
            "786/786 - 3s - loss: 0.0824 - precision: 0.9604 - val_loss: 0.0941 - val_precision: 0.7087 - 3s/epoch - 4ms/step\n",
            "Epoch 91/500\n",
            "786/786 - 2s - loss: 0.0859 - precision: 0.9580 - val_loss: 0.1014 - val_precision: 0.7018 - 2s/epoch - 3ms/step\n",
            "Epoch 92/500\n",
            "786/786 - 2s - loss: 0.0960 - precision: 0.9583 - val_loss: 0.1086 - val_precision: 0.6949 - 2s/epoch - 3ms/step\n",
            "Epoch 93/500\n",
            "786/786 - 2s - loss: 0.0846 - precision: 0.9589 - val_loss: 0.0957 - val_precision: 0.7116 - 2s/epoch - 3ms/step\n",
            "Epoch 94/500\n",
            "786/786 - 2s - loss: 0.0844 - precision: 0.9592 - val_loss: 0.1023 - val_precision: 0.7013 - 2s/epoch - 3ms/step\n",
            "Epoch 95/500\n",
            "786/786 - 2s - loss: 0.0831 - precision: 0.9593 - val_loss: 0.0963 - val_precision: 0.7157 - 2s/epoch - 3ms/step\n",
            "Epoch 96/500\n",
            "786/786 - 3s - loss: 0.0874 - precision: 0.9574 - val_loss: 0.0983 - val_precision: 0.7059 - 3s/epoch - 4ms/step\n",
            "Epoch 97/500\n",
            "786/786 - 2s - loss: 0.0841 - precision: 0.9590 - val_loss: 0.0964 - val_precision: 0.7136 - 2s/epoch - 3ms/step\n",
            "Epoch 98/500\n",
            "786/786 - 2s - loss: 0.0837 - precision: 0.9596 - val_loss: 0.0970 - val_precision: 0.7102 - 2s/epoch - 3ms/step\n",
            "Epoch 99/500\n",
            "786/786 - 3s - loss: 0.0863 - precision: 0.9577 - val_loss: 0.0948 - val_precision: 0.7174 - 3s/epoch - 4ms/step\n",
            "Epoch 100/500\n",
            "786/786 - 2s - loss: 0.0850 - precision: 0.9595 - val_loss: 0.0973 - val_precision: 0.7089 - 2s/epoch - 3ms/step\n",
            "Epoch 101/500\n",
            "Restoring model weights from the end of the best epoch: 81.\n",
            "786/786 - 3s - loss: 0.0844 - precision: 0.9590 - val_loss: 0.0970 - val_precision: 0.7134 - 3s/epoch - 3ms/step\n",
            "Epoch 101: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "display(IPython.display.IFrame(\"https://dagshub.com/\"+ os.environ['MLFLOW_TRACKING_USERNAME'] \n",
        "                        + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + \"/experiments/#/\",'100%',600))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "_uFcjZY6Am0J",
        "outputId": "939d5e50-347f-4710-cd93-f8be13582a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f26eb147ad0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600\"\n",
              "            src=\"https://dagshub.com/vedantd1998/ML/experiments/#/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}